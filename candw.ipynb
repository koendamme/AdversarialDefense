{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-17T09:35:17.639869Z",
     "start_time": "2024-01-17T09:35:11.971045400Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class AdversarialDataset(Dataset):\n",
    "    def __init__(self, annotation_file, categories_file, img_dir, x_transform=None, y_transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        annotations = pd.read_csv(annotation_file)\n",
    "        self.categories = pd.read_csv(categories_file)\n",
    "        self.images = annotations[\"ImageId\"]\n",
    "        self.labels = annotations[\"TrueLabel\"]\n",
    "        self.x_transform = x_transform\n",
    "        self.y_transform = y_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx] + \".png\")\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.x_transform:\n",
    "            image = self.x_transform(image)\n",
    "\n",
    "        return image, self.labels[idx] - 1, self.images[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T09:35:17.648267900Z",
     "start_time": "2024-01-17T09:35:17.642759900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class TensorToDevice(object):\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        \n",
    "    def __call__(self, image):\n",
    "        image = image.to(self.device)\n",
    "        return image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T09:35:17.653947900Z",
     "start_time": "2024-01-17T09:35:17.645267500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def show_image(datarow):\n",
    "    permuted = torch.permute(datarow[0], (1, 2, 0))\n",
    "    plt.title(datarow[1])\n",
    "    plt.imshow(permuted)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T09:35:17.653947900Z",
     "start_time": "2024-01-17T09:35:17.652444100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T09:35:17.707807500Z",
     "start_time": "2024-01-17T09:35:17.655948300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    TensorToDevice(device),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T09:35:17.707807500Z",
     "start_time": "2024-01-17T09:35:17.689597600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T09:35:19.176584400Z",
     "start_time": "2024-01-17T09:35:19.168584700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "dataset = AdversarialDataset(\"images.csv\", \"categories.csv\", \"images\", x_transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=False) #, pin_memory=True)\n",
    "\n",
    "model = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "model = model.to(device)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T09:35:20.013052900Z",
     "start_time": "2024-01-17T09:35:19.380637100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Training</h1>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "n_epochs = 0\n",
    "running_loss = 0\n",
    "n_correct = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in tqdm(dataloader):\n",
    "        x_batch = x_batch.float()\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.type(torch.LongTensor)\n",
    "        y_batch = y_batch.to(device)\n",
    "    \n",
    "        output, _ = model(x_batch)\n",
    "    \n",
    "        loss = loss_function(output, y_batch)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        running_loss += loss.item() + x_batch.size(0)\n",
    "    \n",
    "        _, preds = torch.max(output, dim=1)\n",
    "        n_correct += torch.sum(preds == y_batch.data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T19:06:30.109113Z",
     "start_time": "2024-01-16T19:06:30.103540900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Testing</h1>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# n_correct = 0\n",
    "# \n",
    "# for x_batch, y_batch in tqdm(dataloader):\n",
    "#     x_batch = x_batch.to(device)\n",
    "#     y_batch = y_batch.to(device)\n",
    "# \n",
    "#     output, _ = model(x_batch)\n",
    "# \n",
    "#     _, preds = torch.max(output, dim=1)\n",
    "#     n_correct += torch.sum(preds == y_batch)\n",
    "#     \n",
    "# print(n_correct/len(dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T19:06:30.115327400Z",
     "start_time": "2024-01-16T19:06:30.107605Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h3> Generate C&W images </h3>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/32 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f17b47192824840b51f716f6899b4ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cw_impl.cw as cw\n",
    "\n",
    "def generate_cw_samples(model, dataloader, save_to_disk=True, save_dir=\"adversarial_images_gpu\"):\n",
    "    inputs_box = (min((0 - m) / s for m, s in zip(mean, std)), max((1 - m) / s for m, s in zip(mean, std)))\n",
    "    \n",
    "    adversary = cw.L2Adversary(targeted=False, \n",
    "                               confidence=0.0,\n",
    "                               search_steps=10,\n",
    "                               abort_early=True,\n",
    "                               optimizer_lr=5e-4,\n",
    "                               box=inputs_box)\n",
    "    \n",
    "    model.eval()\n",
    "    for inputs, targets, input_ids in tqdm(dataloader):\n",
    "        # inputs = inputs.to(device)\n",
    "        # targets = targets.to(device)\n",
    "        \n",
    "        adversarial_examples = adversary(model, inputs, targets, to_numpy=False)\n",
    "        \n",
    "        if save_to_disk:\n",
    "            for i in range(adversarial_examples.shape[0]):\n",
    "                # Save as numpy array instead of .png to work around loss of data when scaling\n",
    "                with open(os.path.join(save_dir, input_ids[i] + \".npy\"), 'wb') as f:\n",
    "                    np.save(f, adversarial_examples[i])\n",
    "        \n",
    "generate_cw_samples(model, dataloader)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-17T13:39:42.973121400Z",
     "start_time": "2024-01-17T09:35:33.957307400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_batch(inputs, outputs, device):\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = outputs.to(device)\n",
    "    \n",
    "    output = model(inputs)\n",
    "\n",
    "    _, preds = torch.max(output, dim=1)\n",
    "    print(preds)\n",
    "    return torch.sum(preds == outputs)/inputs.shape[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-16T19:07:47.344307400Z",
     "start_time": "2024-01-16T19:07:47.340306800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-16T19:07:47.340306800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
